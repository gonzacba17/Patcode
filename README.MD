ğŸ§  PatCode â€” Tu asistente de programaciÃ³n local impulsado por Ollama

PatCode es un asistente de programaciÃ³n local que replica las funcionalidades de Claude Code, pero completamente offline, utilizando modelos de Ollama (como Llama 3.2).
Te permite interactuar con tu cÃ³digo, pedir explicaciones, recibir sugerencias o depurar errores directamente desde tu terminal, sin depender de la nube.

ğŸš€ CaracterÃ­sticas principales

ğŸ’¬ InteracciÃ³n natural: escribÃ­ preguntas o instrucciones como si hablaras con otro programador.

ğŸ§  Memoria persistente: recuerda el contexto de las Ãºltimas conversaciones.

ğŸ” AnÃ¡lisis de cÃ³digo: podÃ©s copiar cÃ³digo o fragmentos y pedirle explicaciones o mejoras.

âš™ï¸ 100% local: funciona sin conexiÃ³n, usando tu modelo Ollama (ej. llama3.2:latest).

ğŸ”’ Privado y seguro: ningÃºn dato sale de tu equipo.

ğŸ“ Estructura del proyecto
PatCode/
â”‚
â”œâ”€â”€ main.py                     # Punto de entrada
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pat_agent.py            # LÃ³gica del asistente IA
â”‚
â”œâ”€â”€ memory/
â”‚   â””â”€â”€ memory.json             # Memoria persistente (Ãºltimos diÃ¡logos)
â”‚
â””â”€â”€ requirements.txt            # Dependencias del proyecto

âš™ï¸ Requisitos previos

Antes de comenzar, asegurate de tener instalado:

Python 3.9 o superior

Ollama (en ejecuciÃ³n en tu sistema)
ğŸ‘‰ Descargar Ollama

VerificÃ¡ que Ollama estÃ© corriendo correctamente:

ollama list


Ejemplo de salida:

NAME               ID              SIZE      MODIFIED
llama3.2:latest    a80c4f17acd5    2.0 GB    5 days ago

ğŸ§© InstalaciÃ³n

ClonÃ¡ o creÃ¡ el proyecto:

mkdir PatCode && cd PatCode


Crea un entorno virtual:

python -m venv venv
venv\Scripts\activate      # En Windows
# source venv/bin/activate # En Linux/Mac


Crea los archivos base:

GuardÃ¡ los siguientes archivos en tu carpeta:

requirements.txt
requests

agents/pat_agent.py
import requests
import json

class PatAgent:
    def __init__(self, model="llama3.2:latest", memory_path="memory/memory.json"):
        self.model = model
        self.memory_path = memory_path
        self.history = self.load_memory()

    def load_memory(self):
        try:
            with open(self.memory_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            return []

    def save_memory(self):
        with open(self.memory_path, "w", encoding="utf-8") as f:
            json.dump(self.history, f, indent=2, ensure_ascii=False)

    def ask(self, prompt):
        self.history.append({"role": "user", "content": prompt})

        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": self.model,
                "prompt": self._build_context(),
                "stream": False
            }
        )

        data = response.json()
        answer = data.get("response", "").strip()

        self.history.append({"role": "assistant", "content": answer})
        self.save_memory()
        return answer

    def _build_context(self):
        context = ""
        for msg in self.history[-5:]:
            context += f"{msg['role'].capitalize()}: {msg['content']}\n"
        return context

main.py
from agents.pat_agent import PatAgent

def main():
    print("ğŸ¤– Bienvenido a PatCode (versiÃ³n local con Ollama)\n")
    agent = PatAgent()

    while True:
        prompt = input("TÃº: ")
        if prompt.lower() in ["salir", "exit", "quit"]:
            print("ğŸ‘‹ Hasta luego.")
            break

        answer = agent.ask(prompt)
        print(f"PatCode: {answer}\n")

if __name__ == "__main__":
    main()


InstalÃ¡ dependencias:

pip install -r requirements.txt


EjecutÃ¡ el asistente:

python main.py

ğŸ’¬ Ejemplo de uso
ğŸ¤– Bienvenido a PatCode (versiÃ³n local con Ollama)

TÃº: Explicame cÃ³mo usar listas en Python
PatCode: Las listas en Python son estructuras de datos mutables...

ğŸ§± Arquitectura bÃ¡sica
Usuario  â†’  PatCode (Python)
         â†’  API de Ollama (localhost:11434)
         â†’  Modelo LLM (Llama3.2, etc.)


El flujo de comunicaciÃ³n es local:

main.py recibe tu entrada.

pat_agent.py envÃ­a la solicitud a http://localhost:11434/api/generate.

Ollama procesa con el modelo y devuelve la respuesta.

ğŸŒŸ PrÃ³ximas versiones (en desarrollo)

âœ… VersiÃ³n actual: interacciÃ³n en terminal + memoria bÃ¡sica
ğŸš§ PrÃ³ximas mejoras:

Soporte para anÃ¡lisis de proyectos completos (lectura de carpetas)

EjecuciÃ³n de comandos locales (compilar, testear, etc.)

IntegraciÃ³n con Git (commits automÃ¡ticos y PRs)

Resumen automÃ¡tico de sesiones largas

Interfaz visual (opcional con Streamlit o Electron)

ğŸ¤ Contribuciones

Â¡Toda mejora es bienvenida!
PodÃ©s:

Abrir un issue para sugerencias o errores

Crear un pull request con mejoras

ğŸ§¾ Licencia

MIT License Â© 2025 â€” Creado por Gonza Cba
Inspirado en Claude Code
 de Anthropic.
Implementado con â¤ï¸ sobre Ollama + Python.