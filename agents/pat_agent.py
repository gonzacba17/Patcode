"""
PatAgent - Agente principal de PatCode con capacidades agentic
"""
import json
import requests
from typing import Dict, Any, Optional, List
from pathlib import Path

from config.settings import (
    OLLAMA_BASE_URL, OLLAMA_MODEL, MAX_HISTORY_MESSAGES,
    STREAM_RESPONSES, REQUEST_TIMEOUT, MEMORY_FILE
)
from config.prompts import SYSTEM_PROMPT
from tools.file_tools import ReadFileTool, WriteFileTool, ListDirectoryTool
from tools.shell_tools import ExecuteCommandTool, SearchFilesTool


class PatAgent:
    """
    Agente de programaci√≥n con capacidades agentic
    
    Puede trabajar en dos modos:
    - Simple: Responde directamente (ask)
    - Agentic: Usa herramientas autom√°ticamente (ask_agentic)
    """
    
    def __init__(self, model: str = OLLAMA_MODEL, verbose: bool = False):
        """
        Inicializa el agente
        
        Args:
            model: Nombre del modelo Ollama a usar
            verbose: Si es True, muestra logs detallados
        """
        self.model = model
        self.verbose = verbose
        self.history = self._load_memory()
        self.tools = self._initialize_tools()
        self.current_context = {}  # Contexto de la conversaci√≥n actual
        
        # Agregar system prompt si no existe
        if not self.history or self.history[0].get("role") != "system":
            self.history.insert(0, {
                "role": "system",
                "content": SYSTEM_PROMPT
            })
        
        if self.verbose:
            print(f"ü§ñ PatAgent inicializado con modelo: {self.model}")
    
    def _initialize_tools(self) -> Dict[str, Any]:
        """Inicializa todas las herramientas disponibles"""
        tools = {
            "read_file": ReadFileTool(),
            "write_file": WriteFileTool(),
            "list_directory": ListDirectoryTool(),
            "execute_command": ExecuteCommandTool(),
            "search_files": SearchFilesTool()
        }
        
        if self.verbose:
            print(f"üîß {len(tools)} herramientas cargadas")
        
        return tools
    
    def _load_memory(self) -> List[Dict[str, str]]:
        """
        Carga el historial de conversaci√≥n desde memoria
        
        Returns:
            Lista de mensajes del historial
        """
        try:
            if MEMORY_FILE.exists():
                with open(MEMORY_FILE, "r", encoding="utf-8") as f:
                    history = json.load(f)
                    if self.verbose:
                        print(f"üìö Memoria cargada: {len(history)} mensajes")
                    return history
            return []
        except Exception as e:
            print(f"‚ö†Ô∏è Error cargando memoria: {e}")
            return []
    
    def _save_memory(self):
        """Guarda el historial de conversaci√≥n en memoria"""
        try:
            # Mantener system prompt + √∫ltimos N mensajes
            system_msg = None
            if self.history and self.history[0].get("role") == "system":
                system_msg = self.history[0]
            
            # Tomar los √∫ltimos mensajes
            recent_msgs = self.history[-MAX_HISTORY_MESSAGES:]
            
            # Asegurar que el system prompt est√© primero
            if system_msg and (not recent_msgs or recent_msgs[0] != system_msg):
                self.history = [system_msg] + [m for m in recent_msgs if m != system_msg]
            else:
                self.history = recent_msgs
            
            # Guardar a archivo
            with open(MEMORY_FILE, "w", encoding="utf-8") as f:
                json.dump(self.history, f, indent=2, ensure_ascii=False)
            
            if self.verbose:
                print(f"üíæ Memoria guardada: {len(self.history)} mensajes")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error guardando memoria: {e}")
    
    def _check_ollama_connection(self) -> bool:
        """
        Verifica que Ollama est√© corriendo y accesible
        
        Returns:
            True si Ollama est√° disponible
        """
        try:
            response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _call_ollama(self, prompt: str, stream: bool = STREAM_RESPONSES) -> Optional[str]:
        """
        Llama a la API de Ollama para generar una respuesta
        
        Args:
            prompt: El prompt completo a enviar
            stream: Si es True, muestra la respuesta en tiempo real
            
        Returns:
            Respuesta del modelo o None si hay error
        """
        try:
            response = requests.post(
                f"{OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": stream,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                    }
                },
                timeout=REQUEST_TIMEOUT,
                stream=stream
            )
            
            if stream:
                # Modo streaming: mostrar respuesta en tiempo real
                full_response = ""
                print("PatCode: ", end="", flush=True)
                
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            chunk = data.get("response", "")
                            full_response += chunk
                            print(chunk, end="", flush=True)
                        except json.JSONDecodeError:
                            continue
                
                print()  # Nueva l√≠nea al final
                return full_response
            else:
                # Modo no-streaming: respuesta completa
                data = response.json()
                return data.get("response", "").strip()
                
        except requests.exceptions.Timeout:
            return "‚è±Ô∏è Timeout: La solicitud tard√≥ demasiado. Intent√° con una pregunta m√°s simple."
        except requests.exceptions.ConnectionError:
            return "üîå Error de conexi√≥n: ¬øOllama est√° corriendo? Ejecut√° 'ollama serve'"
        except Exception as e:
            return f"‚ùå Error inesperado: {str(e)}"
    
    def _build_prompt(self) -> str:
        """
        Construye el prompt completo con el historial de conversaci√≥n
        
        Returns:
            String con el prompt formateado
        """
        # Excluir el system prompt del historial visible
        messages = [
            msg for msg in self.history[1:] 
            if msg.get("role") in ["user", "assistant"]
        ]
        
        prompt = ""
        for msg in messages[-MAX_HISTORY_MESSAGES:]:
            role = msg["role"].capitalize()
            content = msg["content"]
            prompt += f"{role}: {content}\n"
        
        return prompt
    
    def use_tool(self, tool_name: str, **kwargs) -> Dict[str, Any]:
        """
        Ejecuta una herramienta espec√≠fica
        
        Args:
            tool_name: Nombre de la herramienta a ejecutar
            **kwargs: Argumentos para la herramienta
            
        Returns:
            Diccionario con el resultado de la ejecuci√≥n
        """
        if tool_name not in self.tools:
            return {
                "success": False,
                "error": f"Herramienta desconocida: {tool_name}",
                "available_tools": list(self.tools.keys())
            }
        
        if self.verbose:
            print(f"üîß Ejecutando: {tool_name} con {kwargs}")
        
        tool = self.tools[tool_name]
        result = tool.execute(**kwargs)
        
        if self.verbose:
            status = "‚úÖ" if result.get("success") else "‚ùå"
            print(f"{status} Resultado: {result.get('message', result.get('error'))}")
        
        return result
    
    def ask(self, user_input: str, stream: bool = STREAM_RESPONSES) -> str:
        """
        Procesa una pregunta en modo simple (sin agentic loop)
        
        Args:
            user_input: Pregunta o comando del usuario
            stream: Si mostrar la respuesta en streaming
            
        Returns:
            Respuesta del modelo
        """
        # Verificar conexi√≥n con Ollama
        if not self._check_ollama_connection():
            return "‚ùå Error: Ollama no est√° corriendo. Ejecut√° 'ollama serve' en otra terminal."
        
        # Agregar mensaje del usuario al historial
        self.history.append({
            "role": "user",
            "content": user_input
        })
        
        # Construir prompt y obtener respuesta
        prompt = self._build_prompt()
        response = self._call_ollama(prompt, stream=stream)
        
        if response:
            # Agregar respuesta al historial
            self.history.append({
                "role": "assistant",
                "content": response
            })
            
            # Guardar memoria
            self._save_memory()
        
        return response if not stream else ""
    
    def ask_agentic(self, user_input: str, stream: bool = True) -> str:
        """
        Procesa una pregunta usando el agentic loop
        El agente puede usar herramientas autom√°ticamente
        
        Args:
            user_input: Pregunta o tarea del usuario
            stream: Si mostrar respuestas en streaming
            
        Returns:
            Respuesta final del agente
        """
        # Importar aqu√≠ para evitar circular import
        from agents.agentic_loop import AgenticLoop
        
        # Verificar conexi√≥n
        if not self._check_ollama_connection():
            return "‚ùå Error: Ollama no est√° corriendo. Ejecut√° 'ollama serve' en otra terminal."
        
        # Agregar mensaje del usuario al historial
        self.history.append({
            "role": "user",
            "content": user_input
        })
        
        # Ejecutar agentic loop
        loop = AgenticLoop(self, verbose=self.verbose)
        result = loop.run(user_input, stream=stream)
        
        return result
    
    def clear_history(self):
        """Limpia el historial de conversaci√≥n conservando el system prompt"""
        system_msg = None
        if self.history and self.history[0].get("role") == "system":
            system_msg = self.history[0]
        
        self.history = [system_msg] if system_msg else []
        self._save_memory()
        
        print("üóëÔ∏è Historial limpiado")
    
    def list_tools(self) -> List[Dict[str, str]]:
        """
        Devuelve informaci√≥n de todas las herramientas disponibles
        
        Returns:
            Lista de diccionarios con nombre y descripci√≥n de cada herramienta
        """
        return [
            {
                "name": name,
                "description": tool.description
            }
            for name, tool in self.tools.items()
        ]
    
    def get_history_summary(self) -> Dict[str, Any]:
        """
        Devuelve un resumen del historial actual
        
        Returns:
            Diccionario con estad√≠sticas del historial
        """
        user_msgs = sum(1 for msg in self.history if msg.get("role") == "user")
        assistant_msgs = sum(1 for msg in self.history if msg.get("role") == "assistant")
        
        return {
            "total_messages": len(self.history),
            "user_messages": user_msgs,
            "assistant_messages": assistant_msgs,
            "model": self.model
        }
    
    def export_conversation(self, filepath: str = "conversation_export.json"):
        """
        Exporta la conversaci√≥n actual a un archivo
        
        Args:
            filepath: Ruta donde guardar el export
        """
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump({
                    "model": self.model,
                    "timestamp": str(Path(filepath).stat().st_mtime),
                    "history": self.history
                }, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Conversaci√≥n exportada a: {filepath}")
        except Exception as e:
            print(f"‚ùå Error exportando: {e}")