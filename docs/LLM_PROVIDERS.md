# Sistema H√≠brido de LLM Providers - PatCode

## üìã Introducci√≥n

PatCode ahora soporta m√∫ltiples proveedores de modelos de lenguaje (LLM) con fallback autom√°tico. Esto significa que puedes usar diferentes servicios seg√∫n tus necesidades y disponibilidad, con cambio autom√°tico si uno falla.

## ü§ñ Providers Disponibles

### 1. **Groq** (Recomendado para producci√≥n)

**Pros:**
- ‚ö° Extremadamente r√°pido (inferencia acelerada por hardware)
- üÜì Free tier generoso
- üéØ Modelos optimizados (Llama 3.1, Mixtral)
- üìä Rate limits razonables (30 req/min)

**Contras:**
- üîë Requiere API key
- üåê Requiere conexi√≥n a internet
- üí≥ L√≠mites en tier gratuito

**Modelos recomendados:**
- `llama-3.1-70b-versatile` (por defecto) - Balance perfecto
- `llama-3.1-8b-instant` - M√°s r√°pido, menos potente
- `mixtral-8x7b-32768` - Contexto muy largo

**C√≥mo obtener API key:**
1. Ve a [console.groq.com](https://console.groq.com)
2. Crea una cuenta gratis
3. Ve a "API Keys"
4. Crea una nueva key
5. Agr√©gala a tu `.env`: `GROQ_API_KEY=gsk_...`

### 2. **Ollama** (Recomendado para desarrollo local)

**Pros:**
- üè† 100% local y privado
- üÜì Completamente gratis
- üîí Sin l√≠mites de rate
- üì° Funciona sin internet

**Contras:**
- üêå M√°s lento que Groq (especialmente sin GPU)
- üíæ Requiere RAM (2-8GB seg√∫n modelo)
- ‚öôÔ∏è Requiere instalaci√≥n local

**Modelos recomendados:**
- `qwen2.5-coder:1.5b` (por defecto) - R√°pido, poco RAM
- `qwen2.5-coder:7b` - Mejor calidad, m√°s RAM
- `codellama:7b` - Especializado en c√≥digo
- `deepseek-coder:6.7b` - Excelente para c√≥digo

**Instalaci√≥n:**
```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Descarga desde https://ollama.com/download

# Verificar instalaci√≥n
ollama --version

# Descargar modelo
ollama pull qwen2.5-coder:1.5b

# Iniciar servidor
ollama serve
```

### 3. **OpenAI** (Opcional)

**Pros:**
- üéØ GPT-4 y modelos de √∫ltima generaci√≥n
- üìö Amplio conocimiento
- üîß API muy confiable

**Contras:**
- üí∞ Requiere pago (no hay tier gratuito real)
- üåê Requiere internet
- üîë Requiere API key de pago

**Modelos recomendados:**
- `gpt-4o-mini` (por defecto) - Balance costo/calidad
- `gpt-4-turbo` - M√°xima calidad
- `gpt-3.5-turbo` - M√°s econ√≥mico

**C√≥mo obtener API key:**
1. Ve a [platform.openai.com](https://platform.openai.com)
2. Crea una cuenta
3. Configura m√©todo de pago
4. Ve a "API Keys"
5. Crea una nueva key
6. Agr√©gala a tu `.env`: `OPENAI_API_KEY=sk-...`

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno (.env)

```bash
# ========================================
# LLM PROVIDER CONFIGURATION
# ========================================

# Provider principal (groq | ollama | openai)
LLM_DEFAULT_PROVIDER=groq

# Fallback autom√°tico si falla el principal
LLM_AUTO_FALLBACK=true

# Orden de fallback (separado por comas)
LLM_FALLBACK_ORDER=groq,ollama

# ========================================
# GROQ SETTINGS
# ========================================
GROQ_API_KEY=gsk_your_key_here
GROQ_MODEL=llama-3.1-70b-versatile
GROQ_TIMEOUT=30

# ========================================
# OLLAMA SETTINGS
# ========================================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5-coder:1.5b
OLLAMA_TIMEOUT=30

# ========================================
# OPENAI SETTINGS (Opcional)
# ========================================
OPENAI_API_KEY=sk_your_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_TIMEOUT=30

# ========================================
# GENERAL LLM SETTINGS
# ========================================
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1024
```

### Configuraci√≥n Program√°tica

```python
from config import settings

# Acceder a configuraci√≥n LLM
print(settings.llm.default_provider)
print(settings.llm.groq_api_key)
print(settings.llm.auto_fallback)
```

## üöÄ Uso

### Comandos CLI

```bash
# Ver providers disponibles y estado
/llm providers

# Cambiar provider manualmente
/llm switch groq
/llm switch ollama

# Ver estad√≠sticas de uso
/llm stats

# Probar un provider espec√≠fico
/llm test groq
/llm test ollama
```

### Ejemplos de Uso

#### 1. Solo Ollama (sin API keys)

```bash
# .env
LLM_DEFAULT_PROVIDER=ollama
OLLAMA_MODEL=qwen2.5-coder:1.5b

# Iniciar
ollama serve
python main.py
```

PatCode usar√° Ollama autom√°ticamente.

#### 2. Groq con Ollama como fallback

```bash
# .env
LLM_DEFAULT_PROVIDER=groq
LLM_AUTO_FALLBACK=true
LLM_FALLBACK_ORDER=groq,ollama
GROQ_API_KEY=gsk_...

# Iniciar
python main.py
```

Si Groq falla (sin internet, rate limit, etc.), autom√°ticamente usa Ollama.

#### 3. M√∫ltiples providers con prioridad

```bash
# .env
LLM_DEFAULT_PROVIDER=groq
LLM_FALLBACK_ORDER=groq,openai,ollama
GROQ_API_KEY=gsk_...
OPENAI_API_KEY=sk-...
```

Prioridad: Groq ‚Üí OpenAI ‚Üí Ollama

## üîÑ Fallback Autom√°tico

El sistema de fallback funciona as√≠:

1. **Intenta con provider principal**
   ```
   Usuario: Hola
   ‚Üí Intenta con Groq...
   ```

2. **Si falla, loguea warning**
   ```
   [WARNING] Provider 'groq' fall√≥: Connection timeout
   [INFO] Intentando fallback a 'ollama'...
   ```

3. **Intenta con siguiente en orden**
   ```
   ‚Üí Intenta con Ollama...
   ‚úì Fallback exitoso a 'ollama'
   ```

4. **Si todos fallan, retorna error explicativo**
   ```
   [ERROR] No hay providers de fallback disponibles
   Sugerencias:
   - Verifica tu GROQ_API_KEY
   - Aseg√∫rate que Ollama est√© corriendo: ollama serve
   ```

### Desactivar Fallback

Si quieres que falle en lugar de hacer fallback:

```bash
LLM_AUTO_FALLBACK=false
```

## üìä Monitoreo

### Ver Provider Actual

```python
# En PatAgent
print(agent.llm_manager.get_current_provider())
# Output: "groq"
```

### Ver Estad√≠sticas

```bash
/llm stats
```

Output:
```
üìä Estad√≠sticas LLM:

  Provider actual: groq
  Providers disponibles: groq, ollama
  Auto-fallback: ‚úÖ

üìà Por Provider:

  GROQ:
    - Requests: 45
    - Exitosos: 43
    - Fallidos: 2
    - Tasa de √©xito: 95.6%
    - Tiempo promedio: 1.23s

  OLLAMA:
    - Requests: 2
    - Exitosos: 2
    - Fallidos: 0
    - Tasa de √©xito: 100%
    - Tiempo promedio: 4.56s
```

## üõ†Ô∏è Troubleshooting

### Groq: "API key inv√°lida"

```bash
# Verificar que la key est√© bien configurada
echo $GROQ_API_KEY

# Debe empezar con "gsk_"
# Si no funciona, genera una nueva en console.groq.com
```

### Ollama: "Connection refused"

```bash
# Verificar que Ollama est√© corriendo
curl http://localhost:11434/api/tags

# Si no responde, iniciar servidor
ollama serve

# Verificar modelos instalados
ollama list

# Si el modelo no est√°, descargarlo
ollama pull qwen2.5-coder:1.5b
```

### "No hay providers disponibles"

```bash
# Verificar configuraci√≥n
/llm providers

# Debe mostrar al menos uno con ‚úÖ

# Si todos tienen ‚ùå:
# 1. Verifica .env
# 2. Verifica que Ollama est√© corriendo
# 3. Verifica tus API keys
```

### Rate Limit en Groq

Groq tiene l√≠mite de 30 requests/minuto en tier gratuito.

Si ves: `Rate limit alcanzado, esperando X.Xs...`

El sistema autom√°ticamente:
- Espera el tiempo necesario
- O hace fallback a Ollama si est√° habilitado

### Modelo no encontrado en Ollama

```bash
# Error: "Modelo 'xxx' no encontrado"
# Soluci√≥n:
ollama pull qwen2.5-coder:1.5b

# Ver modelos disponibles
ollama list
```

## üéØ Casos de Uso Recomendados

### Desarrollo Local (sin internet)

```bash
LLM_DEFAULT_PROVIDER=ollama
OLLAMA_MODEL=qwen2.5-coder:1.5b
```

### Producci√≥n (m√°xima velocidad)

```bash
LLM_DEFAULT_PROVIDER=groq
LLM_FALLBACK_ORDER=groq,ollama
GROQ_API_KEY=gsk_...
GROQ_MODEL=llama-3.1-70b-versatile
```

### M√°xima disponibilidad

```bash
LLM_DEFAULT_PROVIDER=groq
LLM_AUTO_FALLBACK=true
LLM_FALLBACK_ORDER=groq,openai,ollama
GROQ_API_KEY=gsk_...
OPENAI_API_KEY=sk-...
```

### Solo OpenAI (alta calidad)

```bash
LLM_DEFAULT_PROVIDER=openai
LLM_AUTO_FALLBACK=false
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
```

## üìà Comparaci√≥n de Performance

| Provider | Velocidad | Calidad | Costo | Internet | Privacidad |
|----------|-----------|---------|-------|----------|------------|
| **Groq** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Gratis* | ‚úÖ | ‚ö†Ô∏è |
| **Ollama** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Gratis | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **OpenAI** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞ | ‚úÖ | ‚ö†Ô∏è |

*Free tier con l√≠mites

## üîê Seguridad

- **Nunca** compartas tus API keys
- **Nunca** commitees `.env` a git (ya est√° en `.gitignore`)
- Usa variables de entorno en producci√≥n
- Rota tus keys peri√≥dicamente
- Usa Ollama si trabajas con c√≥digo sensible

## üÜò Soporte

Si tienes problemas:

1. Revisa esta documentaci√≥n
2. Ejecuta `/llm providers` para ver el estado
3. Revisa los logs de PatCode
4. Verifica tu configuraci√≥n en `.env`
5. Abre un issue en GitHub con los logs

## üöÄ Pr√≥ximas Mejoras

- [ ] Soporte para Anthropic Claude
- [ ] Soporte para Google Gemini
- [ ] Sistema de cach√© por provider
- [ ] Balance de carga entre providers
- [ ] M√©tricas de costo por provider
- [ ] UI web para gesti√≥n de providers
