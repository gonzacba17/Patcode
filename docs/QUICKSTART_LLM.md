# üöÄ Gu√≠a R√°pida: Sistema LLM en PatCode

## Instalaci√≥n en 5 minutos

### 1. Instalar dependencias

```bash
pip install -r requirements.txt
```

Esto instalar√°:
- `groq` - Cliente de Groq API
- `openai` - Cliente de OpenAI (opcional)

### 2. Configurar provider

**Opci√≥n A: Usar Ollama (100% local, sin API keys)**

```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelo
ollama pull qwen2.5-coder:1.5b

# Iniciar servidor
ollama serve
```

Archivo `.env`:
```bash
LLM_DEFAULT_PROVIDER=ollama
```

**Opci√≥n B: Usar Groq (gratis, r√°pido, requiere internet)**

1. Ve a [console.groq.com](https://console.groq.com)
2. Crea cuenta gratis
3. Genera API key

Archivo `.env`:
```bash
LLM_DEFAULT_PROVIDER=groq
GROQ_API_KEY=gsk_tu_key_aqui
```

**Opci√≥n C: Groq + Ollama (fallback autom√°tico)**

```bash
LLM_DEFAULT_PROVIDER=groq
LLM_AUTO_FALLBACK=true
LLM_FALLBACK_ORDER=groq,ollama
GROQ_API_KEY=gsk_tu_key_aqui
```

### 3. Iniciar PatCode

```bash
python main.py
```

## Comandos B√°sicos

```bash
# Ver providers disponibles
/llm providers

# Cambiar de provider
/llm switch groq
/llm switch ollama

# Ver estad√≠sticas
/llm stats

# Probar un provider
/llm test groq
```

## Ejemplo de Sesi√≥n

```
$ python main.py

PatCode v0.4 ü§ñ
LLM Provider: groq
Modelo: llama-3.1-70b-versatile

> Hola
¬°Hola! Soy Pat, tu asistente de programaci√≥n. ¬øEn qu√© puedo ayudarte?

> /llm providers
ü§ñ LLM Providers:

  ‚úÖ groq ‚¨Ö ACTUAL
  ‚úÖ ollama

üí° Auto-fallback: ‚úÖ Activado
üìã Orden de fallback: groq, ollama

> /llm switch ollama
‚úÖ Provider cambiado a: ollama

> Hola de nuevo
¬°Hola! Ahora estoy usando Ollama localmente. ¬øQu√© necesitas?
```

## Troubleshooting R√°pido

### "No hay providers disponibles"

```bash
# Verificar Ollama
curl http://localhost:11434/api/tags

# Si falla, iniciar Ollama
ollama serve
```

### "Groq API key inv√°lida"

```bash
# Verificar que est√© configurada
echo $GROQ_API_KEY

# Debe empezar con "gsk_"
# Si no, genera una nueva en console.groq.com
```

### "Modelo no encontrado"

```bash
# Descargar modelo en Ollama
ollama pull qwen2.5-coder:1.5b

# Ver modelos instalados
ollama list
```

## Configuraci√≥n Recomendada

Para la mejor experiencia:

```bash
# .env
LLM_DEFAULT_PROVIDER=groq
LLM_AUTO_FALLBACK=true
LLM_FALLBACK_ORDER=groq,ollama

GROQ_API_KEY=gsk_tu_key
GROQ_MODEL=llama-3.1-70b-versatile

OLLAMA_MODEL=qwen2.5-coder:1.5b

LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=1024
```

Esto te da:
- ‚ö° Velocidad de Groq cuando hay internet
- üè† Ollama como backup local
- üîÑ Cambio autom√°tico si Groq falla

## Siguiente Paso

Lee la [documentaci√≥n completa](LLM_PROVIDERS.md) para:
- Configuraci√≥n avanzada
- Comparaci√≥n de providers
- Optimizaci√≥n de performance
- Manejo de errores
