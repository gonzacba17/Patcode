# üß† PatCode - AetherMind Edition

> Asistente de programaci√≥n local impulsado por IA - 100% Offline con Ollama

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](Dockerfile)
[![Ollama](https://img.shields.io/badge/ollama-compatible-green)](https://ollama.ai)

---

## üìñ √çndice

1. [Introducci√≥n](#-introducci√≥n)
2. [Caracter√≠sticas Principales](#-caracter√≠sticas-principales)
3. [Instalaci√≥n R√°pida](#-instalaci√≥n-r√°pida)
4. [Uso](#-uso)
5. [Arquitectura](#-arquitectura)
6. [Configuraci√≥n](#-configuraci√≥n)
7. [Desarrollo](#-desarrollo)
8. [Estado del Proyecto](#-estado-del-proyecto)

---

## üéØ Introducci√≥n

**PatCode AetherMind** es un asistente de programaci√≥n local que replica funcionalidades avanzadas de Claude Code, pero **completamente offline** usando modelos LLM locales via [Ollama](https://ollama.ai/).

### ¬øPor qu√© PatCode?

- **üîí 100% Privado** - Todo corre en tu m√°quina, tus datos nunca salen
- **‚ö° Sin l√≠mites** - Sin rate limiting, cuotas ni suscripciones
- **üåê Offline First** - Funciona sin internet (excepto providers cloud opcionales)
- **üß© Extensible** - Sistema de plugins para funcionalidades personalizadas
- **üé® Multi-Provider** - Soporta Ollama, OpenAI, Groq

---

## ‚ú® Caracter√≠sticas Principales

### üí¨ Conversaci√≥n Inteligente
- Memoria contextual con rotaci√≥n activa/pasiva
- Res√∫menes autom√°ticos de conversaciones largas
- Historial persistente entre sesiones
- B√∫squeda en conversaciones pasadas

### üß© Sistema de Plugins
- **Auto-descubrimiento** de plugins en `plugins/builtin/`
- **Plugins incluidos:**
  - `CodeExplainer` - Explica c√≥digo en detalle
  - `GitHelper` - Asistencia con Git (status, diff, commits)
  - `FileAnalyzer` - Analiza estructura de proyectos
- **Extensible:** Crea tus propios plugins f√°cilmente

### üîå Multi-Provider LLM
- **Ollama** (local, privado)
- **OpenAI** (GPT-4, GPT-3.5)
- **Groq** (Llama, Mixtral ultrarr√°pido)
- Abstracci√≥n unificada con `BaseAdapter`

### ‚ö° Performance
- **Cach√© inteligente** con similitud Jaccard
- **TTL configurable** (default: 24h)
- **Hit rate t√≠pico:** 35-40% en sesiones largas
- **Ahorro:** ~50% en queries repetidas

### üìä Monitoreo
- **Telemetr√≠a simple:** counters, gauges, timers
- **Logging avanzado** con rotaci√≥n de archivos
- **Estad√≠sticas de uso:** `/stats` en tiempo real

### üé® Interfaz Moderna
- **Rich Terminal UI:** syntax highlighting, paneles, tablas
- **Autocompletado** con historial persistente
- **Progress bars** para operaciones largas
- **Markdown rendering** de respuestas

---

## üöÄ Instalaci√≥n R√°pida

### Opci√≥n 1: Script Autom√°tico (Recomendado)

```bash
# Clonar repositorio
git clone https://github.com/gonzacba17/Patocode.git
cd Patocode

# Ejecutar instalador
./install.sh
```

El script detecta tu sistema operativo y te gu√≠a paso a paso.

### Opci√≥n 2: Docker

```bash
# Iniciar con docker-compose
docker-compose up -d

# Usar PatCode
docker-compose exec patcode python main.py

# Ver logs
docker-compose logs -f patcode
```

### Opci√≥n 3: Manual

```bash
# Crear entorno virtual
python3 -m venv venv
source venv/bin/activate  # Linux/macOS
# o
venv\Scripts\activate     # Windows

# Instalar dependencias
pip install -r requirements.txt

# Configurar
cp .env.example .env

# Instalar Ollama (si no lo tienes)
# https://ollama.ai

# Descargar modelo
ollama pull qwen2.5-coder:7b

# Ejecutar
python main.py
```

---

## üíª Uso

### Comandos Disponibles

```bash
# Comandos b√°sicos
/help              # Muestra ayuda completa
/model [nombre]    # Cambia modelo LLM (si implementado)
/clear             # Limpia memoria activa
/exit              # Sale del programa

# Gesti√≥n de contexto
/load <archivo>    # Carga archivo al contexto
/files             # Lista archivos cargados
/unload <archivo>  # Descarga archivo del contexto
/show <archivo>    # Muestra contenido de archivo cargado

# Memoria y estad√≠sticas
/stats             # Muestra estad√≠sticas de uso
/search <texto>    # Busca en historial
/export [archivo]  # Exporta conversaci√≥n a JSON

# Plugins
/plugins           # Gestiona plugins
/cache             # Gestiona cach√© de respuestas
```

### Ejemplos de Uso

#### 1. Pregunta Simple
```
T√∫: ¬øQu√© es un decorador en Python?
PatCode: Un decorador en Python es una funci√≥n que modifica el comportamiento...
```

#### 2. Explicar C√≥digo
```
T√∫: Explica este c√≥digo:
def factorial(n):
    return 1 if n == 0 else n * factorial(n-1)

PatCode: Esta funci√≥n calcula el factorial de un n√∫mero usando recursi√≥n...
```

#### 3. An√°lisis de Proyecto
```
T√∫: /load README.md
PatCode: ‚úÖ Archivo cargado al contexto

T√∫: Analiza la estructura del proyecto
PatCode: Bas√°ndome en el README, este proyecto tiene...
```

#### 4. Usar Plugins
```
T√∫: /plugins
PatCode: [Muestra tabla de plugins disponibles]

T√∫: Usa git_helper para ver el status
PatCode: [Ejecuta git status y muestra an√°lisis]
```

---

## üèóÔ∏è Arquitectura

```
PatCode/
‚îú‚îÄ‚îÄ agents/                      # L√≥gica principal
‚îÇ   ‚îú‚îÄ‚îÄ llm_adapters/           # Abstracci√≥n de providers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_adapter.py     # Interfaz base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_adapter.py   # Adapter Ollama
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_adapter.py   # Adapter OpenAI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ groq_adapter.py     # Adapter Groq
‚îÇ   ‚îú‚îÄ‚îÄ memory/                 # Sistema de memoria
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory_manager.py   # Gestor principal
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sqlite_memory_manager.py  # Persistencia SQLite
‚îÇ   ‚îú‚îÄ‚îÄ cache/                  # Sistema de cach√©
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cache_manager.py    # Gestor de cach√©
‚îÇ   ‚îú‚îÄ‚îÄ pat_agent.py            # Agente principal
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py         # Orquestador de flujos
‚îÇ
‚îú‚îÄ‚îÄ plugins/                     # Sistema de plugins
‚îÇ   ‚îú‚îÄ‚îÄ base.py                 # PluginInterface, PluginManager
‚îÇ   ‚îú‚îÄ‚îÄ registry.py             # Registro de plugins
‚îÇ   ‚îî‚îÄ‚îÄ builtin/                # Plugins integrados
‚îÇ       ‚îú‚îÄ‚îÄ code_explainer.py   # Explicador de c√≥digo
‚îÇ       ‚îú‚îÄ‚îÄ git_helper.py       # Helper de Git
‚îÇ       ‚îî‚îÄ‚îÄ file_analyzer.py    # Analizador de archivos
‚îÇ
‚îú‚îÄ‚îÄ ui/                         # Interfaz de usuario
‚îÇ   ‚îú‚îÄ‚îÄ rich_terminal.py        # Terminal UI con Rich
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                  # CLI con Click
‚îÇ   ‚îî‚îÄ‚îÄ memory_commands.py      # Comandos de memoria
‚îÇ
‚îú‚îÄ‚îÄ utils/                      # Utilidades
‚îÇ   ‚îú‚îÄ‚îÄ simple_telemetry.py     # Telemetr√≠a
‚îÇ   ‚îú‚îÄ‚îÄ response_cache.py       # Cach√© de respuestas
‚îÇ   ‚îú‚îÄ‚îÄ logger.py               # Sistema de logging
‚îÇ   ‚îî‚îÄ‚îÄ validators.py           # Validadores
‚îÇ
‚îú‚îÄ‚îÄ config/                     # Configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ settings.py             # Settings centralizados
‚îÇ   ‚îú‚îÄ‚îÄ model_selector.py       # Selector de modelos
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py              # Prompts del sistema
‚îÇ
‚îú‚îÄ‚îÄ tests/                      # Tests (3200+ l√≠neas)
‚îÇ   ‚îú‚îÄ‚îÄ test_*.py               # Tests unitarios
‚îÇ   ‚îî‚îÄ‚îÄ integration/            # Tests de integraci√≥n
‚îÇ
‚îú‚îÄ‚îÄ scripts/                    # Scripts DevOps
‚îÇ   ‚îú‚îÄ‚îÄ setup.sh                # Setup inicial
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh               # Despliegue
‚îÇ   ‚îî‚îÄ‚îÄ backup.sh               # Backup de datos
‚îÇ
‚îú‚îÄ‚îÄ docs/                       # Documentaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ README.MD               # Este archivo
‚îÇ   ‚îú‚îÄ‚îÄ CHANGELOG.md            # Historial de cambios
‚îÇ   ‚îú‚îÄ‚îÄ LLM_PROVIDERS.md        # Gu√≠a de providers
‚îÇ   ‚îî‚îÄ‚îÄ QUICKSTART_LLM.md       # Quickstart LLM
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile                  # Containerizaci√≥n
‚îú‚îÄ‚îÄ docker-compose.yml          # Orquestaci√≥n Docker
‚îú‚îÄ‚îÄ install.sh                  # Instalador autom√°tico
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias
‚îî‚îÄ‚îÄ .env.example                # Configuraci√≥n ejemplo
```

---

## ‚öôÔ∏è Configuraci√≥n

### Archivo `.env`

```bash
# ================================
# OLLAMA - Configuraci√≥n LLM Local
# ================================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5-coder:7b
OLLAMA_TEMPERATURE=0.7
REQUEST_TIMEOUT=120

# ================================
# PROVIDERS CLOUD (OPCIONAL)
# ================================
OPENAI_API_KEY=sk-...
GROQ_API_KEY=gsk_...

# ================================
# MEMORIA - Persistencia
# ================================
MAX_HISTORY_MESSAGES=20
CONTEXT_WINDOW_SIZE=10
MAX_ACTIVE_MESSAGES=10
MAX_MEMORY_FILE_SIZE=5242880  # 5MB

# ================================
# CACH√â
# ================================
CACHE_ENABLED=true
CACHE_TTL_SECONDS=86400  # 24 horas

# ================================
# LOGGING
# ================================
LOG_LEVEL=INFO
ENABLE_FILE_LOGGING=true
LOG_MAX_BYTES=10485760  # 10MB
LOG_BACKUP_COUNT=5
```

### Modelos Recomendados

| Modelo | RAM Min | RAM Rec | Velocidad | Uso Ideal |
|--------|---------|---------|-----------|-----------|
| `qwen2.5-coder:7b` | 8GB | 12GB | ‚ö°‚ö° R√°pido | **Coding general (RECOMENDADO)** |
| `llama3.2:3b` | 6GB | 8GB | ‚ö°‚ö°‚ö° Muy r√°pido | Chat, preguntas simples |
| `codellama:7b` | 12GB | 16GB | ‚ö° Balanceado | Debugging, refactoring |
| `deepseek-coder:6.7b` | 10GB | 14GB | ‚ö°‚ö° R√°pido | Code completion, an√°lisis |
| `mistral:7b` | 12GB | 16GB | ‚ö° Balanceado | General purpose |

---

## üß™ Desarrollo

### Instalar Dependencias de Desarrollo

```bash
pip install -r requirements-dev.txt
```

### Ejecutar Tests

```bash
# Todos los tests
pytest tests/

# Con cobertura
pytest --cov=agents --cov=plugins --cov=utils tests/

# Tests espec√≠ficos
pytest tests/test_llm_system.py -v

# Con reporte HTML
pytest --cov=agents --cov-report=html tests/
```

### Pre-commit Hooks

```bash
# Instalar hooks
pre-commit install

# Ejecutar manualmente
pre-commit run --all-files
```

### Linting y Formateo

```bash
# Black (formateo)
black agents/ plugins/ utils/

# isort (ordenar imports)
isort agents/ plugins/ utils/

# flake8 (linting)
flake8 agents/ plugins/ utils/
```

---

## üìä Estado del Proyecto

### Versi√≥n Actual: `1.0.0-beta` ‚úÖ **FUNCIONAL**

**Progreso General: ~90% Completado** üéâ

#### ‚úÖ Completado (Fases 1, 2 y 3)

- ‚úÖ **Fase 1 - Fundamentos (100%)**
  - Sistema de configuraci√≥n externalizada
  - Manejo robusto de errores
  - Logging avanzado
  - Healthcheck de Ollama
  - Validadores de entrada

- ‚úÖ **Fase 2 - Arquitectura Multi-Provider (90%)**
  - ‚úÖ Abstracci√≥n de providers (BaseAdapter)
  - ‚úÖ **Adapters funcionales:** Ollama, OpenAI, Groq (3/3)
  - ‚úÖ Sistema de memoria optimizado con rotaci√≥n
  - ‚úÖ Comandos especiales implementados
  - ‚úÖ **Bugs cr√≠ticos resueltos** (2025-10-21)
  - üöß Streaming de respuestas (pendiente)
  - üöß Comando `/model` din√°mico (pendiente)

- ‚úÖ **Fase 3 - Avanzado (100%)**
  - Sistema de plugins extensible
  - 3 plugins built-in funcionales
  - Cach√© inteligente con TTL
  - Telemetr√≠a simple y avanzada
  - Containerizaci√≥n completa (Docker)
  - CI/CD configurado (GitHub Actions)
  - Scripts DevOps (4 scripts)

#### üêõ Bugs Conocidos

- ‚ö†Ô∏è **Warning de logger** (no cr√≠tico) - PathLike error al arrancar
- üìã **Todos los bugs cr√≠ticos resueltos** (21-Oct-2025)

#### üìã Pendiente (Fase 4)

- [ ] Implementar streaming de respuestas (3-4h)
- [ ] Comando `/model` para cambio din√°mico (2h)
- [ ] Completar tests de integraci√≥n (>70% cobertura)
- [ ] Ejecutar CI/CD pipeline completo
- [ ] Resolver warning de logger

### M√©tricas Reales

- **Archivos Python:** 161
- **L√≠neas de c√≥digo:** 34,279
- **Tests:** 17 archivos (3,020 l√≠neas)
- **Plugins built-in:** 3/3 funcionales
- **Adapters funcionales:** 3/3 ‚úÖ
- **Providers soportados:** 3
- **Comandos CLI:** ~12
- **Dependencias:** 21
- **Bugs cr√≠ticos:** 0 ‚úÖ

Ver [ROADMAP.md](ROADMAP.md) para detalles completos.

---

## ü§ù Contribuir

¬°Las contribuciones son bienvenidas!

1. Fork el repositorio
2. Crea una rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -m "feat: nueva funcionalidad"`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Crea un Pull Request

### Convenciones

- **Commits:** [Conventional Commits](https://www.conventionalcommits.org/)
  - `feat:` - Nueva funcionalidad
  - `fix:` - Correcci√≥n de bug
  - `docs:` - Documentaci√≥n
  - `test:` - Tests
  - `refactor:` - Refactorizaci√≥n

- **C√≥digo:** PEP 8
- **Docstrings:** Google Style
- **Type hints:** Preferidos

---

## üìù Licencia

MIT License - Ver [LICENSE](../LICENSE)

Copyright (c) 2025 Gonza Cba

---

## üôè Agradecimientos

- [Anthropic](https://anthropic.com) - Inspiraci√≥n con Claude Code
- [Ollama](https://ollama.ai) - Motor LLM local
- [Rich](https://rich.readthedocs.io) - UI de terminal
- Comunidad open-source

---

## üìß Contacto y Soporte

- **GitHub Issues:** [github.com/gonzacba17/Patocode/issues](https://github.com/gonzacba17/Patocode/issues)
- **Repositorio:** [github.com/gonzacba17/Patocode](https://github.com/gonzacba17/Patocode)
- **Autor:** Gonza Cba

---

## üìö Recursos Adicionales

- [Documentaci√≥n Ollama](https://github.com/ollama/ollama)
- [LLM Providers Guide](LLM_PROVIDERS.md)
- [Quickstart LLM](QUICKSTART_LLM.md)
- [Performance Guide](performance_guide.md)
- [Changelog Completo](CHANGELOG.md)

---

**Creado con ‚ù§Ô∏è por Gonza Cba**

_Si te gusta PatCode, ¬°dale una ‚≠ê en GitHub!_
