# Gu√≠a de Performance - PatCode v0.4.0

## Sistema de Cache

### ¬øC√≥mo funciona?

PatCode genera un hash √∫nico basado en:
- √öltimos 5 mensajes de conversaci√≥n
- Archivos cargados en contexto

Si detecta una query id√©ntica, usa la respuesta cacheada en lugar de consultar al LLM.

### Configuraci√≥n

**TTL (Time To Live):**
- Default: 24 horas
- Modificable en `utils/response_cache.py`

**Directorio:**
- Default: `.patcode_cache/`
- Se crea autom√°ticamente
- Agregar a `.gitignore`

### Comandos
```bash
# Ver estad√≠sticas
patcode cache stats

# Limpiar expirado
patcode cache clean

# Limpiar todo
patcode cache clear
```

### M√©tricas

**Hit Rate T√≠pico:**
- Conversaciones cortas (<10 mensajes): 10-20%
- Conversaciones medias (10-30 mensajes): 30-40%
- Sesiones de refactoring: 40-50%

**Ahorro de Tiempo:**
- Cache hit: ~2s ‚Üí ~0.1s (95% m√°s r√°pido)
- Modelos lentos se benefician m√°s

---

## Selecci√≥n de Modelos

### Estrategia de Selecci√≥n

PatCode recomienda modelos seg√∫n:

1. **RAM Disponible:**
   - Usa 80% de RAM libre como l√≠mite
   - Margen de seguridad del 20%

2. **Caso de Uso:**
   - Quick questions ‚Üí modelo ligero
   - General coding ‚Üí modelo balanceado
   - Refactoring ‚Üí modelo profundo

3. **Performance:**
   - M√°s RAM disponible ‚Üí modelos m√°s grandes
   - RAM limitada ‚Üí modelos optimizados

### Recomendaciones

**4-6 GB RAM:**
```bash
patcode chat --model llama3.2:1b
```

**8-12 GB RAM:**
```bash
patcode chat --model llama3.2:3b  # Default
```

**16+ GB RAM:**
```bash
patcode chat --model codellama:13b --deep
```

**Auto-selecci√≥n:**
```bash
patcode chat --auto  # Detecta y recomienda
```

### Benchmarks

| Modelo | Tokens/s | Memoria | Latencia |
|--------|----------|---------|----------|
| llama3.2:1b | ~80 | 4GB | ~1s |
| llama3.2:3b | ~50 | 8GB | ~2s |
| codellama:7b | ~30 | 12GB | ~3s |
| codellama:13b | ~15 | 16GB | ~5s |

*Benchmarks en CPU (Intel i7). GPU acelera 3-5x*

---

## Optimizaci√≥n Avanzada

### 1. Cache Agresivo

Para proyectos repetitivos, puedes extender el TTL editando `utils/response_cache.py`:
```python
cache = ResponseCache(ttl_hours=48)  # 2 d√≠as
```

### 2. Modelo H√≠brido
```bash
# Quick questions con modelo r√°pido
patcode chat --fast

# Refactoring con modelo profundo
patcode refactor main.py --deep
```

### 3. Pre-calentamiento
```bash
# Cargar modelo en memoria
ollama run llama3.2:3b "hello"

# Luego usar PatCode
patcode chat
```

### 4. Monitoreo
```bash
# Ver stats en tiempo real
watch -n 1 'patcode cache stats'

# Ver uso de RAM
watch -n 1 'free -h'
```

---

## Troubleshooting

### Cache no funciona

**S√≠ntoma:** Hit rate siempre 0%

**Soluci√≥n:**
```bash
# Verificar directorio
ls -la .patcode_cache/

# Verificar permisos
chmod 755 .patcode_cache

# Limpiar y recrear
patcode cache clear
```

### Modelo muy lento

**S√≠ntoma:** Respuestas tardan >10s

**Soluci√≥n:**
```bash
# Verificar RAM disponible
patcode models

# Cambiar a modelo m√°s ligero
patcode chat --fast

# Verificar CPU/GPU
htop
```

### OOM (Out of Memory)

**S√≠ntoma:** Sistema se congela

**Soluci√≥n:**
```bash
# Usar modelo m√°s peque√±o
patcode chat --model llama3.2:1b

# Verificar swap disponible
free -h
```

---

## Tips Pro

1. **Usa `--auto` por defecto** - Deja que PatCode optimice
2. **Revisa cache stats semanalmente** - Limpia si crece mucho
3. **Modelos espec√≠ficos para tareas espec√≠ficas:**
   - Chat ‚Üí llama3.2:3b
   - Code ‚Üí codellama:7b
   - Docs ‚Üí mistral:7b
4. **Cache hit rate bajo?** - Normal si cambias mucho de contexto
5. **GPU disponible?** - Ollama lo usa autom√°ticamente (3-5x m√°s r√°pido)

---

## Casos de Uso

### Desarrollo Activo
```bash
# Sesi√≥n larga con cache habilitado
patcode chat --auto

# Ver mejoras en tiempo real
patcode cache stats
```

### Testing/Debugging
```bash
# Sin cache para respuestas frescas
patcode chat --no-cache --fast
```

### An√°lisis Profundo
```bash
# Modelo potente con cache
patcode analyze . --deep
```

### Integraci√≥n CI/CD
```bash
# Modelo ligero, sin cache
patcode chat --model llama3.2:1b --no-cache
```

---

## M√©tricas de √âxito

**v0.4.0 vs v0.3.1:**
- ‚ö° 50% m√°s r√°pido en queries repetidas
- üíæ 30% reducci√≥n uso de LLM
- üß† Hit rate promedio: 37%
- üìä Ahorro acumulado: ~2 horas/semana en sesiones largas

**Objetivo v0.5.0:**
- ‚ö° 60% reducci√≥n latencia
- üíæ Hit rate >50%
- üß† Predicci√≥n de modelos seg√∫n tarea
