"""
Test de Componentes Existentes
Prueba funcionalidad de componentes que ya estÃ¡n implementados
"""

import sys
from pathlib import Path

def test_cli_commands():
    """Test del sistema de comandos CLI"""
    print("\n" + "="*60)
    print("ğŸ§ª TESTING: CLI Commands")
    print("="*60)
    
    try:
        from cli.commands import CommandRegistry
        
        registry = CommandRegistry()
        
        # Test 1: Comandos bÃ¡sicos registrados
        print("\nâœ“ Test 1: Verificar comandos bÃ¡sicos")
        basic_commands = ['help', 'clear', 'exit', 'files', 'stats']
        for cmd in basic_commands:
            if cmd in registry.commands:
                print(f"  âœ… Comando '{cmd}' registrado")
            else:
                print(f"  âŒ Comando '{cmd}' NO registrado")
        
        # Test 2: Get help
        print("\nâœ“ Test 2: Sistema de ayuda")
        help_text = registry.get_help()
        if "PatCode" in help_text and "Comandos Disponibles" in help_text:
            print("  âœ… Sistema de ayuda funciona")
        else:
            print("  âŒ Sistema de ayuda tiene problemas")
        
        # Test 3: Comando especÃ­fico
        print("\nâœ“ Test 3: Ayuda de comando especÃ­fico")
        help_files = registry.get_help("files")
        if "/files" in help_files:
            print("  âœ… Ayuda especÃ­fica funciona")
        else:
            print("  âŒ Ayuda especÃ­fica tiene problemas")
        
        print("\nâœ… CLI Commands: OK")
        return True
        
    except Exception as e:
        print(f"\nâŒ CLI Commands: FAILED - {e}")
        import traceback
        traceback.print_exc()
        return False


def test_cli_formatter():
    """Test del formateador de output"""
    print("\n" + "="*60)
    print("ğŸ§ª TESTING: CLI Formatter")
    print("="*60)
    
    try:
        from cli.formatter import OutputFormatter, Colors
        
        formatter = OutputFormatter(use_colors=True)
        
        # Test 1: Formato de respuesta
        print("\nâœ“ Test 1: Formato de respuesta con markdown")
        test_text = """# Header
## Subheader
- Item 1
- Item 2
`inline code`
"""
        formatted = formatter.format_response(test_text)
        if formatted and len(formatted) > 0:
            print("  âœ… Formato de respuesta funciona")
        else:
            print("  âŒ Formato de respuesta tiene problemas")
        
        # Test 2: Tabla
        print("\nâœ“ Test 2: Formato de tabla")
        headers = ["Name", "Age", "City"]
        rows = [["Alice", "30", "NYC"], ["Bob", "25", "LA"]]
        table = formatter.format_table(headers, rows)
        if "Alice" in table and "â”‚" in table:
            print("  âœ… Formato de tabla funciona")
        else:
            print("  âŒ Formato de tabla tiene problemas")
        
        # Test 3: Code block
        print("\nâœ“ Test 3: Formato de bloque de cÃ³digo")
        code = "def hello():\n    print('Hello')"
        code_block = formatter.format_code_block(code, "python")
        if code_block and "hello" in code_block:
            print("  âœ… Formato de cÃ³digo funciona")
        else:
            print("  âŒ Formato de cÃ³digo tiene problemas")
        
        # Test 4: Info boxes
        print("\nâœ“ Test 4: Info boxes")
        info_box = formatter.format_info_box("Test", "Content", "info")
        success_box = formatter.format_success("Success message")
        error_box = formatter.format_error("Error message")
        
        if all([info_box, success_box, error_box]):
            print("  âœ… Info boxes funcionan")
        else:
            print("  âŒ Info boxes tienen problemas")
        
        print("\nâœ… CLI Formatter: OK")
        return True
        
    except Exception as e:
        print(f"\nâŒ CLI Formatter: FAILED - {e}")
        import traceback
        traceback.print_exc()
        return False


def test_diff_viewer():
    """Test del visualizador de diffs"""
    print("\n" + "="*60)
    print("ğŸ§ª TESTING: Diff Viewer")
    print("="*60)
    
    try:
        from utils.diff_viewer import show_diff
        
        # Test: Generar diff
        print("\nâœ“ Test: Generar diff entre dos textos")
        old = "def hello():\n    print('Hello')"
        new = "def hello(name):\n    print(f'Hello {name}')"
        
        diff = show_diff(old, new, "Original", "Modificado")
        
        if diff and ("Hello" in diff or "hello" in diff):
            print("  âœ… Diff viewer funciona")
            # Mostrar preview del diff
            print("\n  Preview del diff:")
            for line in diff.split('\n')[:10]:
                print(f"    {line}")
        else:
            print("  âŒ Diff viewer tiene problemas")
        
        print("\nâœ… Diff Viewer: OK")
        return True
        
    except Exception as e:
        print(f"\nâŒ Diff Viewer: FAILED - {e}")
        import traceback
        traceback.print_exc()
        return False


def test_file_editor():
    """Test del editor de archivos"""
    print("\n" + "="*60)
    print("ğŸ§ª TESTING: File Editor")
    print("="*60)
    
    try:
        from tools.file_editor import FileEditor
        
        editor = FileEditor(backup_dir=Path('.test_backups'))
        
        # Test 1: Verificar inicializaciÃ³n
        print("\nâœ“ Test 1: InicializaciÃ³n")
        if editor.backup_dir.exists():
            print("  âœ… Backup directory creado")
        else:
            print("  âŒ Backup directory no creado")
        
        # Test 2: Read file (este mismo archivo)
        print("\nâœ“ Test 2: Lectura de archivo")
        success, content, error = editor.read_file(Path(__file__))
        if success and content:
            print(f"  âœ… Lectura OK ({len(content)} chars)")
        else:
            print(f"  âŒ Lectura fallÃ³: {error}")
        
        # Cleanup
        import shutil
        if Path('.test_backups').exists():
            shutil.rmtree('.test_backups')
        
        print("\nâœ… File Editor: OK")
        return True
        
    except Exception as e:
        print(f"\nâŒ File Editor: FAILED - {e}")
        import traceback
        traceback.print_exc()
        return False


def test_project_memory():
    """Test de la memoria del proyecto"""
    print("\n" + "="*60)
    print("ğŸ§ª TESTING: Project Memory")
    print("="*60)
    
    try:
        from agents.memory.project_memory import ProjectMemory
        
        # Usar directorio temporal para tests
        import tempfile
        import shutil
        
        temp_dir = tempfile.mkdtemp()
        
        try:
            memory = ProjectMemory(temp_dir)
            
            # Test 1: Add context
            print("\nâœ“ Test 1: AÃ±adir contexto")
            memory.add_context("test_file.py", "print('hello')")
            if "test_file.py" in memory.context:
                print("  âœ… Contexto aÃ±adido")
            else:
                print("  âŒ Contexto no aÃ±adido")
            
            # Test 2: Add task
            print("\nâœ“ Test 2: AÃ±adir tarea")
            task_id = memory.add_task("Test task", "pending")
            if task_id:
                print(f"  âœ… Tarea aÃ±adida (ID: {task_id})")
            else:
                print("  âŒ Tarea no aÃ±adida")
            
            # Test 3: Get recent tasks
            print("\nâœ“ Test 3: Obtener tareas recientes")
            tasks = memory.get_recent_tasks(5)
            if len(tasks) > 0:
                print(f"  âœ… Tareas obtenidas ({len(tasks)})")
            else:
                print("  âš ï¸  No hay tareas")
            
            # Test 4: Save/Load
            print("\nâœ“ Test 4: Persistencia")
            memory.save()
            
            # Crear nueva instancia y verificar carga
            memory2 = ProjectMemory(temp_dir)
            if "test_file.py" in memory2.context:
                print("  âœ… Datos persistidos correctamente")
            else:
                print("  âŒ Datos no se persistieron")
            
            print("\nâœ… Project Memory: OK")
            return True
            
        finally:
            # Cleanup
            shutil.rmtree(temp_dir, ignore_errors=True)
        
    except Exception as e:
        print(f"\nâŒ Project Memory: FAILED - {e}")
        import traceback
        traceback.print_exc()
        return False


def test_llm_manager():
    """Test del gestor de LLMs"""
    print("\n" + "="*60)
    print("ğŸ§ª TESTING: LLM Manager")
    print("="*60)
    
    try:
        from agents.llm_manager import LLMManager
        
        # Test 1: InicializaciÃ³n
        print("\nâœ“ Test 1: InicializaciÃ³n")
        manager = LLMManager()
        
        # Test 2: Obtener adapters disponibles
        print("\nâœ“ Test 2: Adapters disponibles")
        providers = list(manager.adapters.keys())
        print(f"  ğŸ“‹ Providers: {', '.join(providers)}")
        if len(providers) > 0:
            print(f"  âœ… {len(providers)} adapters cargados")
        else:
            print("  âŒ No hay adapters")
        
        # Test 3: Get current provider
        print("\nâœ“ Test 3: Provider actual")
        current = manager.get_current_provider()
        if current:
            print(f"  âœ… Provider actual: {current}")
        else:
            print("  âŒ No hay provider actual")
        
        # Test 4: Get stats
        print("\nâœ“ Test 4: EstadÃ­sticas")
        stats = manager.get_stats()
        if stats and 'current_provider' in stats:
            print(f"  âœ… Stats OK")
            print(f"    - Current: {stats['current_provider']}")
            print(f"    - Available: {', '.join(stats['available_providers'])}")
        else:
            print("  âŒ Stats tienen problemas")
        
        print("\nâœ… LLM Manager: OK")
        return True
        
    except Exception as e:
        print(f"\nâŒ LLM Manager: FAILED - {e}")
        import traceback
        traceback.print_exc()
        return False


def test_orchestrator():
    """Test del orchestrator"""
    print("\n" + "="*60)
    print("ğŸ§ª TESTING: Orchestrator")
    print("="*60)
    
    try:
        from agents.orchestrator import AgenticOrchestrator
        from agents.llm_manager import LLMManager
        
        # Test 1: InicializaciÃ³n
        print("\nâœ“ Test 1: InicializaciÃ³n")
        
        llm_manager = LLMManager()
        orchestrator = AgenticOrchestrator(
            llm_manager=llm_manager,
            project_root=".",
            max_iterations=3,
            enable_shell=False  # Deshabilitamos shell para test
        )
        
        if orchestrator:
            print("  âœ… Orchestrator inicializado")
        else:
            print("  âŒ Orchestrator no inicializado")
        
        # Test 2: Verificar componentes
        print("\nâœ“ Test 2: Componentes internos")
        components = []
        if hasattr(orchestrator, 'file_ops'):
            components.append("file_ops")
        if hasattr(orchestrator, 'code_analyzer'):
            components.append("code_analyzer")
        if hasattr(orchestrator, 'project_memory'):
            components.append("project_memory")
        
        print(f"  âœ… Componentes: {', '.join(components)}")
        
        # Test 3: Context
        print("\nâœ“ Test 3: Execution Context")
        if hasattr(orchestrator, 'context'):
            print(f"  âœ… Context OK")
            print(f"    - Root: {orchestrator.context.project_root}")
        else:
            print("  âŒ No hay context")
        
        print("\nâœ… Orchestrator: OK")
        return True
        
    except Exception as e:
        print(f"\nâŒ Orchestrator: FAILED - {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_test_report(results):
    """Genera reporte de tests"""
    print("\n\n" + "="*60)
    print("ğŸ“Š REPORTE DE TESTS - COMPONENTES EXISTENTES")
    print("="*60)
    
    passed = sum(1 for r in results.values() if r)
    total = len(results)
    percentage = (passed / total) * 100 if total > 0 else 0
    
    print(f"\nâœ… Tests pasados: {passed}/{total} ({percentage:.1f}%)")
    
    print("\nDetalle:")
    for name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {status} - {name}")
    
    if percentage == 100:
        print("\nğŸ‰ Â¡Todos los tests pasaron!")
    elif percentage >= 70:
        print("\nğŸ‘ La mayorÃ­a de componentes funcionan bien")
    elif percentage >= 40:
        print("\nâš ï¸  Algunos componentes tienen problemas")
    else:
        print("\nğŸ”´ MÃºltiples componentes tienen problemas")
    
    print("\n" + "="*60)


def main():
    """Ejecuta todos los tests"""
    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘     TESTS DE COMPONENTES EXISTENTES - PATOCODE             â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    results = {}
    
    # Ejecutar tests
    results["CLI Commands"] = test_cli_commands()
    results["CLI Formatter"] = test_cli_formatter()
    results["Diff Viewer"] = test_diff_viewer()
    results["File Editor"] = test_file_editor()
    results["Project Memory"] = test_project_memory()
    results["LLM Manager"] = test_llm_manager()
    results["Orchestrator"] = test_orchestrator()
    
    # Reporte
    generate_test_report(results)


if __name__ == "__main__":
    main()
